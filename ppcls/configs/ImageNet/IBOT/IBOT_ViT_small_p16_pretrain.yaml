# global configs
Global:
  checkpoints: null
  pretrained_model: null
  train_mode: ibot

  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
  use_fp16: True
  weight_decay: 0.04
  weight_decay_end: 0.4
  epochs: 800
  freeze_last_layer: 1
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1e-06
  batch_size: 32
#  num_workers: 10
  global_crops_scale: [ 0.25, 1.0 ]
  local_crops_number: 10
  local_crops_scale: [ 0.05, 0.25 ]
  pred_ratio: [0, 0.3]
  pred_ratio_var: [0 0.2]

  seed: 0
  ngpus: 8
  nodes: 2
  device: gpu
#  optimizer: adamw
  momentum_teacher: 0.996
  output_dir: ./output_ibot_vit_small_p16/

  eval_during_train: False
  eval_interval: 1
  print_batch_step: 20
  use_visualdl: True
  save_interval: 1

AMP:
  scale_loss: 128.0
  use_dynamic_loss_scaling: True
  # O1: mixed fp16
  level: O1

# model architecture
Arch:
  name: IBOT
  mode: pretrain
  arch: ViT_small
  patch_size: 16
  out_dim: 8192
  patch_out_dim: 8192
  norm_last_layer: False
  shared_head: True
  class_num: 1000
  drop_path: 0.1
  use_masked_im_modeling: True
  norm_in_head: None
  act_in_head: gelu
  shared_head_teacher: True
  global_crops_number: 2
  global_crops_scale: [ 0.25, 1.0 ]
  local_crops_number: 10
  local_crops_scale: [ 0.05, 0.25 ]
  pred_ratio: [ 0, 0.3 ]
  pred_ratio_var: [ 0 0.2 ]

# loss function config for traing/eval process
Loss:
  Train:
    - IBOTLoss:
        weight: 1.0
        out_dim: 8192
        patch_out_dim: 8192
        # global_crops_number
        ngcrops: 2
        # local_crops_number
        nlcrops: 10
        warmup_teacher_temp: 0.04
        teacher_temp: 0.07
        # warmup_teacher_patch_temp
        warmup_teacher_temp2: 0.04
        # teacher_patch_temp
        teacher_temp2: 0.07
        warmup_teacher_temp_epochs: 30
        nepochs: 800
        lambda1: 1.0
        lambda2: 1.0
        mim_start_epoch: 0
  Eval:
    - CELoss:
        weight: 1.0


Optimizer:
  name: AdamWIBOT
  weight_decay: 0.04
  no_weight_decay_name: norm bias
  clip_norm: 0
  lr:
    # for 8 cards
    name: CosineIBOT
    # 运算之后，注意修改
    base_value: 0.00125
    final_value: 1e-06
    epochs: 800
    step_each_epoch: 1
    warmup_epochs: 10
    start_warmup_value: 0


# data loader for train and eval
DataLoader:
  Train:
    dataset:
      name: IBOTDataset
      image_root: /data3/linkaihao/dataset/mini-imagenet-1k
      cls_label_path: /data3/linkaihao/dataset/mini-imagenet-1k/train_list.txt
      transform_ops:
          - DecodeImage:
              to_np: False
              to_rgb: True
              channel_first: False
              backend: pil
          - IBOTAugmentation:
              global_crops_scale: [ 0.25, 1.0 ]
              local_crops_scale: [ 0.05, 0.25 ]
              global_crops_number: 2
              local_crops_number: 10
    sampler:
      name: DistributedBatchSampler
      batch_size: 32
      drop_last: True
      shuffle: True
    loader:
      num_workers: 10
      use_shared_memory: True

#Infer:
#  infer_imgs: docs/images/inference_deployment/whl_demo.jpg
#  batch_size: 10
#  transforms:
#    - DecodeImage:
#        to_rgb: True
#        channel_first: False
#    - ResizeImage:
#        resize_short: 256
#    - CropImage:
#        size: 224
#    - NormalizeImage:
#        scale: 1.0/255.0
#        mean: [0.485, 0.456, 0.406]
#        std: [0.229, 0.224, 0.225]
#        order: ''
#    - ToCHWImage:
#  PostProcess:
#    name: Topk
#    topk: 5
#    class_id_map_file: ppcls/utils/imagenet1k_label_list.txt
#
#Metric:
#    Train:
#    - DistillationTopkAcc:
#        model_key: "Student"
#        topk: [1, 5]
#    Eval:
#    - DistillationTopkAcc:
#        model_key: "Student"
#        topk: [1, 5]