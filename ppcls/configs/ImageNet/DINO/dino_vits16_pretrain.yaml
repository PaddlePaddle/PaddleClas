# global configs
Global:
  train_mode: dino
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
  use_fp16: False
  weight_decay: 0.04
  weight_decay_end: 0.4
  clip_grad: 0
  epochs: 800
  freeze_last_layer: 1
  lr: 0.0005
  warmup_epochs: 10
  min_lr: 1e-05
  global_crops_scale: [0.25, 1.0]
  local_crops_scale: [0.05, 0.25]
  local_crops_number: 10
  seed: 0
  world_size: 16
  ngpus: 8
  nodes: 2
  device: gpu
  optimizer: adamw
  momentum_teacher: 0.996
  output_dir: ./output/

# model architecture
Arch:
  name: DINO
  mode: pretrain
  arch: ViT_small
  patch_size: 16
  out_dim: 65536
  drop_path_rate: 0.1
  use_bn_in_head: False
  norm_last_layer: False
  class_num: 1000

Optimizer:
  name: AdamW
  no_weight_decay_name: norm bias
  one_dim_param_no_weight_decay: True

# data loader
DataLoader:
  Train:
    dataset:
      name: ImageNetDataset
      image_root: ./dataset/ILSVRC2012/
      cls_label_path: ./dataset/ILSVRC2012/train_list.txt
      transform_ops:
        - DecodeImage:
            to_rgb: True
            channel_first: False
        - DataAugmentationDINO:
            global_crops_scale: [ 0.25, 1.0 ]
            local_crops_scale: [ 0.05, 0.25 ]
            local_crops_number: 10
    sampler:
      name: DistributedBatchSampler
      batch_size: 64
      drop_last: True
      shuffle: True
    loader:
      num_workers: 10


Loss:
  Train:
    - DINOLoss:
        out_dim: 65536
        ncrops: 12   # 10 + 2
        warmup_teacher_temp: 0.04
        teacher_temp: 0.07
        warmup_teacher_temp_epochs: 30
        nepochs: 800
