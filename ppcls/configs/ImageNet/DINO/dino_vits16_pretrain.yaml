# global configs
Global:
  checkpoints: null
  pretrained_model: null

  train_mode: dino
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
  use_fp16: False # ???
  weight_decay: 0.04
  weight_decay_end: 0.4
  epochs: 800
  freeze_last_layer: 1
  lr: 0.0005
  warmup_epochs: 10
  min_lr: 1e-05
  global_crops_scale: [0.25, 1.0]
  local_crops_scale: [0.05, 0.25]
  local_crops_number: 10
  seed: 0
  ngpus: 8
  nodes: 2
  device: gpu
  optimizer: adamw
  momentum_teacher: 0.996
  output_dir: ./output/

  eval_during_train: False
  eval_interval: 1
  print_batch_step: 20
  use_visualdl: True
  save_interval: 1

# model architecture
Arch:
  name: DINO
  mode: pretrain
  arch: ViT_small
  patch_size: 16
  out_dim: 65536
  drop_path_rate: 0.1
  use_bn_in_head: False
  norm_last_layer: False
  class_num: 1000

Optimizer:
  name: AdamW
  weight_decay: 0.04
  no_weight_decay_name: norm bias
  clip_norm: 0
  lr:
    # for 8 cards
    name: Constant
    learning_rate: 5e-4

# data loader
DataLoader:
  Train:
    dataset:
      name: ImageNetDataset
      image_root: ./dataset/ILSVRC2012/
      cls_label_path: ./dataset/ILSVRC2012/train_list.txt
      transform_ops:
        - DecodeImage:
            to_np: False
            to_rgb: True
            channel_first: False
            backend: pil
        - DataAugmentationDINO:
            global_crops_scale: [ 0.25, 1.0 ]
            local_crops_scale: [ 0.05, 0.25 ]
            local_crops_number: 10
    sampler:
      name: DistributedBatchSampler
      batch_size: 64
      drop_last: True
      shuffle: True
    loader:
      num_workers: 10
      use_shared_memory: True


Loss:
  Train:
    - DINOLoss:
        weight: 1.0
        out_dim: 65536
        ncrops: 12   # 10 + 2
        warmup_teacher_temp: 0.04
        teacher_temp: 0.07
        warmup_teacher_temp_epochs: 30
        nepochs: 800
