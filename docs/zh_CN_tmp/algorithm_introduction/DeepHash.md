# DeepHash
## 1. DeepHash简介：
最近邻搜索(Approximate Nearest Neighbor)是计算机视觉、推荐系统和机器学习等许多领域中最基本的问题之一。它的目的是依据一定的距离度量方式找到库中距离待检索特征最近的特征向量。然而，当数据量大且维度高时，准确找到最接近查询的点的时间成本是很大的。为了解决这一问题，人们开始越来越重视近似最近邻搜索，因为在大多数情况下它可以满足搜索需求，且显著降低搜索复杂度。

哈希是最广泛使用的方法之一，因为它在计算和存储方面非常有效。其目的是将高纬度的原始特征转换为低维哈希码，使相似对象的哈希码尽可能接近，不同对象的哈希码尽可能不同。现有的哈希方法主要包括局部敏感哈希和学习哈希。局部敏感哈希的目的是将原始数据映射到多个哈希桶中。对象之间的原始距离越近，落入同一哈希桶的概率越大。通过该机制，提出了许多基于局部敏感哈希的算法, 这些算法在计算和存储方面都有很高的优势。然而，为了提高搜索的召回率，这些方法通常需要构建许多不同的哈希表，因此它们在特别大的数据集上的应用仍然有限。

由于局部敏感哈希是与数据无关的，人们试图通过学习好的哈希函数来获得高质量的哈希代码。自早期的语义哈希和谱哈希这两种算法提出以来，哈希学习在计算机视觉和机器学习领域引起了广泛的研究兴趣。随着深度学习的发展，通过深度学习获得哈希代码越来越受到重视，原因有二。第一个原因是，深度学习强大的表示能力可以学习非常复杂的哈希函数。第二个原因是深度学习可以实现端到端的哈希代码，这在很多应用中非常有用。 

## 2. DeepHash算法分类
DeepHash算法的差异主要体现在两方面： 1） 网络结构的设计  2）损失函数的设计。
### 2.1 按照骨干网络分类
- CNN based: 
    此类方法通常采用某种CNN结构作为骨干网络。对于MINST和CIFAR-10这样的小数据集，浅层架构如AlexNet和CNN-F被广泛使用。对于像NUS-WIDE和COCO这样的复杂数据集，需要更深层次的架构，如VGG和ResNet50。

- Transformer based:
    随着Transformer在各个计算机视觉领域的应用，DeepHash领域也随之开始拥抱Transformer, 如2021年的TransHash, 此类方法相比于CNN based方法，检索精度可以超出10多个点，但模型通常比较大，占用存储比较多。
   
### 2.2 按照相似度保持方式分类
- 两两相似度保持
- 多方向相似度保持
- 隐式相似度保持
- 面向分类保持
- 量化方式保持

## 常用算法介绍
-  DLBHC

-  LCDSH

-  DSHSD


## 参考文献
